{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomadlist Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Importing \"Web Scraping Toolkit\" libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Getting all cities on Nomadlist.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty pandas dataframe with one column \"city\"\n",
    "df = pd.DataFrame(columns=['city'])\n",
    "\n",
    "# Parse local cities.xml file with BeautifulSoup\n",
    "soup = BeautifulSoup(open('data/raw/cities.xml', 'r', encoding='utf-8'), 'html.parser')\n",
    "\n",
    "# Look for and loop over all <li> tags\n",
    "cities = soup.find_all('li', {'class', 'item'})\n",
    "for city in cities:\n",
    "    if city.has_attr('data-slug'): # City name is in the \"data-slug\" attribute\n",
    "        city_name = city['data-slug']\n",
    "\n",
    "        # Add one row to pandas dataframe\n",
    "        df = df.append({'city': city_name}, ignore_index=True)\n",
    "\n",
    "# Save dataframe to cities csv file\n",
    "df.to_csv('data/interim/cities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Getting all users on Nomadlist.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for this scraping task with placeholder\n",
    "base_url = 'https://nomadlist.com/people/{}'\n",
    "\n",
    "# Read dataframe from cities csv file\n",
    "df_cities = pd.read_csv('data/interim/cities.csv')\n",
    "user_list = [] # Empty list for a set of users\n",
    "\n",
    "# Loop over all cities in the dataframe\n",
    "for index, row in df_cities.iterrows():\n",
    "    # Request url with city name appended and parse with BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(base_url.format(row[0]))\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Look for and loop over all <a> tags\n",
    "        users = soup.find_all('a', {'class': 'no-border'})\n",
    "        for user in users:\n",
    "            if user.has_attr('href') and user['href'].startswith('/@'): # User name is in the \"href\" attribute and starts with \"/@\"\n",
    "                user_name = user['href'][1:]\n",
    "                user_list.append(user_name) # Add user name to user list\n",
    "\n",
    "        # Remove duplicate user names (users can visit multiple cities)\n",
    "        user_list = list(set(user_list))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create pandas dataframe from list and save to users csv file\n",
    "df_users = pd.DataFrame(user_list, columns=['user'])\n",
    "df_users.to_csv('data/interim/users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Getting all trips for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra libraries\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Base URL for this scraping task with placeholder\n",
    "base_url = 'https://nomadlist.com/{}'\n",
    "\n",
    "# Regular expression for the \"tripsCoords\" JavaScript variable\n",
    "p_coords = re.compile('var tripsCoords=(.*?);')\n",
    "\n",
    "# Create empty pandas dataframe with 13 columns\n",
    "df_trips = pd.DataFrame(columns=['city', 'city_slug', 'epoch_end', 'epoch_start', 'latitude', 'longitude', 'previous_latitude', 'previous_longitude', 'trip_id', 'trip_length', 'user_image', 'mode', 'user'])\n",
    "\n",
    "# Read dataframe from users csv file\n",
    "df_users = pd.read_csv('data/interim/users.csv')\n",
    "\n",
    "# Loop over all users in the dataframe\n",
    "for index, row in df_users.iterrows():\n",
    "    \n",
    "    # Request url with user name appended and parse with BeautifulSoup\n",
    "    response = requests.get(base_url.format(row[0]))\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Look for and loop over all <script> tags\n",
    "    scripts = soup.find_all('script')\n",
    "    for script in scripts:\n",
    "\n",
    "        # Search for the regular expression inside the <script> tag\n",
    "        match = p_coords.search(script.prettify())\n",
    "        if match:\n",
    "\n",
    "            # If found, turn into Python dictionary\n",
    "            content = json.loads(match.groups()[0])\n",
    "            if content:\n",
    "\n",
    "                # Load the trips into a list of dictionaries\n",
    "                trips = list(content.values())[0]\n",
    "\n",
    "                # Add username to every trip\n",
    "                trips_with_username = [dict(trip, **{'user':row[0]}) for trip in trips]\n",
    "                \n",
    "                # Append list of trips to the trips dataframe\n",
    "                df_trips = df_trips.append(trips_with_username)\n",
    "\n",
    "# Save dataframe to trips csv file\n",
    "df_trips.to_csv('data/interim/trips.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
